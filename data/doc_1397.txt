https://en.wikipedia.org/wiki/MuZero

From Wikipedia, the free encyclopedia This article needs to be updated . Please help update this article to reflect recent events or newly available information. ( May 2022 ) Game-playing artificial intelligence This article is part of the series on Chess programming Board representations 0x88 Bitboards Evaluation functions Artificial neural networks Efficiently updatable neural networks Piece-square tables Handcrafted evaluation functions Deep neural networks Transformers Convolutional neural networks Residual neural networks Attention Tuning and training algorithms Reinforcement learning Supervised learning Unsupervised learning Gradient descent Stochastic gradient descent Local search ( Texel tuning ) Graph and tree search algorithms Minimax Alpha-beta pruning Principal variation search Quiescence search Monte Carlo tree search Chess computers Belle ChessMachine ChipTest Deep Blue Deep Thought HiTech Hydra Mephisto Saitek Chess engines AlphaZero Chess Tiger Crafty Cray Blitz CuckooChess Deep Fritz Dragon by Komodo Chess Fairy-Max Fritz Fruit GNU Chess HIARCS Houdini Ikarus Junior KnightCap Komodo Leela Chess Zero MChess Pro Mittens MuZero Naum REBEL Rybka Shredder Sjeng SmarThink Stockfish Torch Turochamp Zappa v t e MuZero is a computer program developed by artificial intelligence research company DeepMind , a subsidiary of Google to master games without knowing their rules and underlying dynamics. [ 1 ] [ 2 ] [ 3 ] Its release in 2019 included benchmarks of its performance in Go , chess , shogi , and a suite of 57 different Atari games. The algorithm uses an approach similar to AlphaZero , where a combination of a tree-based search and a learned model is deployed. It matched AlphaZero's performance in chess and shogi, improved on its performance in Go , and improved on the state of the art in mastering a suite of 57 Atari games (the Arcade Learning Environment), a visually-complex domain. MuZero was trained via self-play , with no access to rules, opening books, or endgame tablebases. The trained algorithm used the same convolutional and residual architecture as AlphaZero, but with 20 percent fewer computation steps per node in the search tree. [ 4 ] History [ edit ] MuZero really is discovering for itself how to build a model and understand it just from first principles. — David Silver, DeepMind, Wired [ 5 ] On November 19, 2019, the DeepMind team released a preprint introducing MuZero. Derivation from AlphaZero [ edit ] Further information: AlphaZero MuZero (MZ) is a combination of the high-performance planning of the AlphaZero (AZ) algorithm with approaches to model-free reinforcement learning. The combination allows for more efficient training in classical planning regimes, such as Go, while also handling domains with much more complex inputs at each stage, such as visual video games. MuZero was derived directly from AZ code, sharing its rules for setting hyperparameters . Differences between the approaches include: [ 6 ] AZ's planning process uses a simulator . The simulator knows the rules of the game. It has to be explicitly programmed. A neural network then predicts the policy and value of a future position. Perfect knowledge of game rules is used in modeling state transitions in the search tree, actions available at each node, and termination of a branch of the tree. MZ does not have access to the rules, and instead learns one with neural networks. AZ has a single model for the game (from board state to predictions); MZ has separate models for representation of the current state (from board state into its internal embedding), dynamics of states (how actions change representations of board states), and prediction of policy and value of a future position (given a state's representation). MZ's hidden model may be complex, and it may turn out it can host computation; exploring the details of the hidden model in a trained instance of MZ is a topic for future exploration. MZ does not expect a two-player game where winners take all. It works with standard reinforcement-learning scenarios, including single-agent environments with continuous intermediate rewards, possibly of arbitrary magnitude and with time discounting. AZ was designed for two-player games that could be won, drawn, or lost. Comparison with R2D2 [ edit ] The previous state of the art technique for learning to play the suite of Atari games was R2D2, the Recurrent Replay Distributed DQN. [ 7 ] MuZero surpassed both R2D2's mean and median performance across the suite of games, though it did not do better in every game. Training and results [ edit ] MuZero used 16 third-generation tensor processing units (TPUs) for training, and 1000 TPUs for selfplay for board games, with 800 simulations per step and 8 TPUs for training and 32 TPUs for selfplay for Atari games, with 50 simulations per step. AlphaZero used 64 second-generation TPUs for training, and 5000 first-generation TPUs for selfplay. As TPU design has improved (third-generation chips are 2x as powerful individually as second-generation chips, with further advances in bandwidth and networking across chips in a pod), these are comparable training setups. R2D2 was trained for 5 days through 2M training steps. Initial results [ edit ] MuZero matched AlphaZero's performance in chess and shogi after roughly 1 million training steps. It matched AZ's performance in Go after 500,000 training steps and surpassed it by 1 million steps. It matched R2D2's mean and median performance across the Atari game suite after 500 thousand training steps and surpassed it by 1 million steps, though it never performed well on 6 games in the suite. Reactions and related work [ edit ] MuZero was viewed as a significant advancement over AlphaZero, and a generalizable step forward in unsupervised learning techniques. [ 8 ] [ 9 ] The work was seen as advancing understanding of how to compose systems from smaller components, a systems-level development more than a pure machine-learning development. [ 10 ] While only pseudocode was released by the development team, Werner Duvaud produced an open source implementation based on that. [ 11 ] MuZero has been used as a reference implementation in other work, for instance as a way to generate model-based behavior. [ 12 ] In late 2021, a more efficient variant of MuZero was proposed, named EfficientZero. It "achieves 194.3 percent mean human performance and 109.0 percent median performance on the Atari 100k benchmark with only two hours of real-time game experience". [ 13 ] In early 2022, a variant of MuZero was proposed to play stochastic games (for example 2048 , backgammon ), called Stochastic MuZero, which uses afterstate dynamics and chance codes to account for the stochastic nature of the environment when training the dynamics network. [ 14 ] See also [ edit ] General game playing Unsupervised learning References [ edit ] ^ Wiggers, Kyle (20 November 2019). "DeepMind's MuZero teaches itself how to win at Atari, chess, shogi, and Go" . VentureBeat . Retrieved 22 July 2020 . ^ Friedel, Frederic. "MuZero figures out chess, rules and all" . ChessBase GmbH . Retrieved 22 July 2020 . ^ Rodriguez, Jesus. "DeepMind Unveils MuZero, a New Agent that Mastered Chess, Shogi, Atari and Go Without Knowing the Rules" . KDnuggets . Retrieved 22 July 2020 . ^ Schrittwieser, Julian; Antonoglou, Ioannis; Hubert, Thomas; Simonyan, Karen; Sifre, Laurent; Schmitt, Simon; Guez, Arthur; Lockhart, Edward; Hassabis, Demis; Graepel, Thore; Lillicrap, Timothy (2020). "Mastering Atari, Go, chess and shogi by planning with a learned model". Nature . 588 (7839): 604– 609. arXiv : 1911.08265 . Bibcode : 2020Natur.588..604S . doi : 10.1038/s41586-020-03051-4 . PMID 33361790 . S2CID 208158225 . ^ "What AlphaGo Can Teach Us About How People Learn" . Wired . ISSN 1059-1028 . Retrieved 2020-12-25 . ^ Silver, David ; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan ; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017). "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm". arXiv : 1712.01815 [ cs.AI ]. ^ Kapturowski, Steven; Ostrovski, Georg; Quan, John; Munos, Remi; Dabney, Will. RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFORCEMENT LEARNING . ICLR 2019 – via Open Review. ^ Shah, Rohin (27 November 2019). "[AN #75]: Solving Atari and Go with learned game models, and thoughts from a MIRI employee - LessWrong 2.0" . www.lesswrong.com . Retrieved 2020-06-07 . ^ Wu, Jun. "Reinforcement Learning, Deep Learning's Partner" . Forbes . Retrieved 2020-07-15 . ^ "Machine Learning & Robotics: My (biased) 2019 State of the Field" . cachestocaches.com . Retrieved 2020-07-15 . ^ Duvaud, Werner (2020-07-15), werner-duvaud/muzero-general , retrieved 2020-07-15 ^ van Seijen, Harm; Nekoei, Hadi; Racah, Evan; Chandar, Sarath (2020-07-06). "The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning". arXiv : 2007.03158 [ cs.stat ]. ^ Ye, Weirui; Liu, Shaohuai; Kurutach, Thanard; Abbeel, Pieter; Gao, Yang (2021-12-11). "Mastering Atari Games with Limited Data". arXiv : 2111.00210 [ cs.LG ]. ^ Antonoglou, Ioannis; Schrittwieser, Julian; Ozair, Serjil; Hubert, Thomas; Silver, David (2022-01-28). "Planning in Stochastic Environments with a Learned Model" . Retrieved 2023-12-12 . External links [ edit ] Initial MuZero preprint Open source implementations v t e Google AI Google Google Brain Google DeepMind Computer programs AlphaGo Versions AlphaGo (2015) Master (2016) AlphaGo Zero (2017) AlphaZero (2017) MuZero (2019) Competitions Fan Hui (2015) Lee Sedol (2016) Ke Jie (2017) In popular culture AlphaGo (2017) The MANIAC (2023) Other AlphaFold (2018) AlphaStar (2019) AlphaDev (2023) AlphaGeometry (2024) AlphaGenome (2025) Machine learning Neural networks Inception (2014) WaveNet (2016) MobileNet (2017) Transformer (2017) EfficientNet (2019) Gato (2022) Other Quantum Artificial Intelligence Lab TensorFlow Tensor Processing Unit Generative AI Chatbots Assistant (2016) Sparrow (2022) Gemini (2023) Nano Banana (2025) Models BERT (2018) XLNet (2019) T5 (2019) LaMDA (2021) Chinchilla (2022) PaLM (2022) Imagen (2023) Gemini (2023) VideoPoet (2024) Gemma (2024) Veo (2024) Other DreamBooth (2022) NotebookLM (2023) Vids (2024) Gemini Robotics (2025) Antigravity (2025) See also " Attention Is All You Need " Future of Go Summit Generative pre-trained transformer Google Labs Google Pixel Google Workspace Robot Constitution Category Commons v t e Artificial intelligence (AI) History timeline Glossary Companies Projects Concepts Parameter Hyperparameter Loss functions Regression Bias–variance tradeoff Double descent Overfitting Clustering Gradient descent SGD Quasi-Newton method Conjugate gradient method Backpropagation Attention Convolution Normalization Batchnorm Activation Softmax Sigmoid Rectifier Gating Weight initialization Regularization Datasets Augmentation Prompt engineering Reinforcement learning Q-learning SARSA Imitation Policy gradient Diffusion Latent diffusion model Autoregression Adversary RAG Uncanny valley RLHF Self-supervised learning Reflection Recursive self-improvement Hallucination Word embedding Vibe coding Safety ( Alignment ) Applications Machine learning In-context learning Artificial neural network Deep learning Language model Large NMT Reasoning Model Context Protocol Intelligent agent Artificial human companion Humanity's Last Exam Lethal autonomous weapons (LAWs) Generative artificial intelligence (GenAI) (Hypothetical: Artificial general intelligence (AGI) ) (Hypothetical: Artificial superintelligence (ASI) ) Implementations Audio–visual AlexNet WaveNet Human image synthesis HWR OCR Computer vision Speech synthesis 15.ai ElevenLabs Speech recognition Whisper Facial recognition AlphaFold Text-to-image models Aurora DALL-E Firefly Flux GPT Image Ideogram Imagen Midjourney Recraft Stable Diffusion Text-to-video models Dream Machine Runway Gen Hailuo AI Kling Sora Veo Music generation Riffusion Suno AI Udio Text Word2vec Seq2seq GloVe BERT T5 Llama Chinchilla AI PaLM GPT 1 2 3 J ChatGPT 4 4o o1 o3 4.5 4.1 o4-mini 5 5.1 5.2 Claude Gemini Gemini (language model) Gemma Grok LaMDA BLOOM DBRX Project Debater IBM Watson IBM Watsonx Granite PanGu-Σ DeepSeek Qwen Decisional AlphaGo AlphaZero OpenAI Five Self-driving car MuZero Action selection AutoGPT Robot control People Alan Turing Warren Sturgis McCulloch Walter Pitts John von Neumann Christopher D. Manning Claude Shannon Shun'ichi Amari Kunihiko Fukushima Takeo Kanade Marvin Minsky John McCarthy Nathaniel Rochester Allen Newell Cliff Shaw Herbert A. Simon Oliver Selfridge Frank Rosenblatt Bernard Widrow Joseph Weizenbaum Seymour Papert Seppo Linnainmaa Paul Werbos Geoffrey Hinton John Hopfield Jürgen Schmidhuber Yann LeCun Yoshua Bengio Lotfi A. Zadeh Stephen Grossberg Alex Graves James Goodnight Andrew Ng Fei-Fei Li Alex Krizhevsky Ilya Sutskever Oriol Vinyals Quoc V. Le Ian Goodfellow Demis Hassabis David Silver Andrej Karpathy Ashish Vaswani Noam Shazeer Aidan Gomez John Schulman Mustafa Suleyman Jan Leike Daniel Kokotajlo François Chollet Architectures Neural Turing machine Differentiable neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Political Regulation of artificial intelligence Ethics of artificial intelligence Precautionary principle AI alignment EU Artificial Intelligence Act (AI Act) Category Retrieved from " https://en.wikipedia.org/w/index.php?title=MuZero&oldid=1319413251 " Categories : 2019 software AlphaGo Applied machine learning 2019 in artificial intelligence Hidden categories: Wikipedia articles in need of updating from May 2022 All Wikipedia articles in need of updating Articles with short description Short description matches Wikidata